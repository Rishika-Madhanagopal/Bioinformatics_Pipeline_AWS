
# ğŸ§¬ Bioinformatics Pipeline on AWS

This project implements an end-to-end **bioinformatics workflow** for processing genomic data using **AWS Cloud Services**.  
It takes raw sequencing data (FASTQ), performs:

- ğŸ§ª Quality control (FastQC)  
- ğŸ¯ Read alignment (Bowtie2 or BWA)  
- ğŸ§¬ Variant calling (GATK)  
- ğŸ“Š Data analysis (Jupyter Notebook)

It runs on **EC2**, stores data in **Amazon S3**, and optionally automates/parallelizes tasks via **AWS Lambda** and **AWS Batch**.

---

## ğŸ“ Project Structure

```
bioinformatics-pipeline-aws/
â”‚
â”œâ”€â”€ data/                      # Sample genomic data
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ install_tools.sh       # Script to install bioinformatics tools
â”‚   â”œâ”€â”€ run_pipeline.sh        # Main shell script to run the pipeline
â”‚   â””â”€â”€ analysis/
â”‚       â”œâ”€â”€ fastqc_reports/
â”‚       â”œâ”€â”€ sam_files/
â”‚       â””â”€â”€ bam_files/
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ aws_architecture_diagram.png  # AWS architecture diagram
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ s3_upload.sh           # Upload results to S3
â”‚   â””â”€â”€ jupyter_analysis.py    # Analysis notebook for results
â”‚
â””â”€â”€ README.md
```

---

## ğŸš€ Features

- âœ… Quality checks using **FastQC**  
- âœ… Read alignment using **Bowtie2** or **BWA**  
- âœ… Alignment file processing with **Samtools**  
- âœ… Variant calling using **GATK**  
- âœ… Uploading data/results to **Amazon S3**  
- âœ… Visualization & analysis in **Jupyter Notebook**  
- âœ… Cloud-native design using **EC2**, **S3**, **Lambda**, and **AWS Batch**

---

## ğŸ”§ Prerequisites

- AWS account with access to EC2, S3, Lambda, and Batch  
- An EC2 instance (Ubuntu preferred, `t2.medium` or higher)  
- SSH access (e.g., PuTTY or native SSH on Windows)  
- Tools installed using `install_tools.sh`  

---

## âš™ï¸ Setup Guide

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/bioinformatics-pipeline-aws.git
cd bioinformatics-pipeline-aws
```

---

### 2ï¸âƒ£ Upload Genomic Data to S3

Use the AWS CLI to upload your input files to S3:

```bash
aws s3 cp ~/Downloads/raw_data/ s3://your-bucket-name/raw_data/ --recursive
```

---

### 3ï¸âƒ£ Launch EC2 Instance

Use **Ubuntu** as the base AMI.  
Connect via SSH:

- Windows users: Use **PuTTY**
- Linux/macOS/WSL users: Use the native `ssh` command

---

### 4ï¸âƒ£ Install Tools on EC2

```bash
cd pipeline
bash install_tools.sh
```

---

### 5ï¸âƒ£ Run the Bioinformatics Pipeline

```bash
bash run_pipeline.sh
```

This script will:

- Run FastQC for quality control  
- Align reads to reference genome  
- Convert SAM to BAM  
- Call variants using GATK  

---

### 6ï¸âƒ£ Upload Results to S3

```bash
cd ../scripts
bash s3_upload.sh
```

---

## ğŸ“Š Analyze Results in Jupyter

1. Launch Jupyter on your EC2 instance (or locally):

```bash
pip3 install jupyter pandas matplotlib
jupyter notebook
```

2. Open `jupyter_analysis.py`  
3. Run each cell to load and visualize the `.vcf` results.

---

## â˜ï¸ AWS Architecture

```
[S3: Stores data] <--> [EC2: pipeline + Jupyter]
         |
     [Lambda / Batch] (optional automation & scaling)
```

For a visual overview, see `docs/aws_architecture_diagram.png`.
![AWS Architecture](docs/AWS Architecture Diagram.png)

---

## ğŸ™Œ Acknowledgements

- [NCBI SRA](https://www.ncbi.nlm.nih.gov/sra)  
- [1000 Genomes Project](https://www.internationalgenome.org/)  
- [GATK Toolkit](https://gatk.broadinstitute.org/)  
- [Amazon Web Services](https://aws.amazon.com/free) â€“ AWS Free Tier  
